{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "val ids...\n",
      "test ids...\n",
      "Validation set length: 854833\n",
      "Test set length: 1446635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Code from\n",
    "\"Two-Stream Aural-Visual Affect Analysis in the Wild\"\n",
    "Felix Kuhnke and Lars Rumberg and Joern Ostermann\n",
    "Please see https://github.com/kuhnkeF/ABAW2020TNT\n",
    "\"\"\"\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import numpy as np\n",
    "from tsav import TwoStreamAuralVisualModel\n",
    "from aff2compdataset import Aff2CompDataset\n",
    "from write_labelfile import write_labelfile\n",
    "from utils import ex_from_one_hot, split_EX_VA_AU\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from clip_transforms import *\n",
    "import torchvision.utils as vutils\n",
    "batch = 30\n",
    "model_path = '/home/alex/Desktop/TSAV_Sub4_544k.pth.tar' # path to the model\n",
    "result_path = 'trying_2'# path where the result .txt files should be stored\n",
    "database_path = 'aff2_processed/'  # path where the database was created (images, audio...) see create_database.py\n",
    "# should be the same path\n",
    "\n",
    "\n",
    "class SubsetSequentialSampler(Sampler):\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print('cpu selected!')\n",
    "    # model\n",
    "    model = TwoStreamAuralVisualModel(num_channels=4)\n",
    "    modes = model.modes\n",
    "    # load the model\n",
    "    saved_model = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(saved_model['state_dict'])\n",
    "    model = model.to(device)\n",
    "    # disable grad, set to eval\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in model.children():\n",
    "        p.train(False)\n",
    "\n",
    "    # load dataset (first time this takes longer)\n",
    "    dataset = Aff2CompDataset(database_path)\n",
    "    dataset.set_modes(modes)\n",
    "\n",
    "    # select the frames we want to process (we choose VAL and TEST)\n",
    "    testvalids = np.logical_or(dataset.test_ids, dataset.val_ids)\n",
    "    print('Validation set length: ' + str(sum(dataset.val_ids)))\n",
    "    print('Test set length: ' + str(sum(dataset.test_ids)))\n",
    "    sampler = SubsetSequentialSampler(np.nonzero(testvalids)[0])\n",
    "    loader = DataLoader(dataset, batch_size=batch, sampler=sampler, num_workers=8, pin_memory=True, drop_last=False)\n",
    "\n",
    "    output = torch.zeros((len(dataset), 17), dtype=torch.float32)\n",
    "    #labels = torch.zeros((len(dataset), 17), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    # store the predictions so we can skip inference later\n",
    "    # os.makedirs(result_path, exist_ok=True)\n",
    "    # torch.save({'predictions': output}, os.path.join(result_path, 'inference.pkl'))\n",
    "\n",
    "    # load the predictions\n",
    "    #output = torch.load(os.path.join(result_path, 'inference.pkl')['predictions']\n",
    "\n",
    "    # VALIDATION RESULTS\n",
    "    # export the results as txt files\n",
    "    # print('writing text files validation')\n",
    "    # o_p_v = os.path.join(result_path, 'val')\n",
    "    # for i in range(len(dataset.val_video_indices)):\n",
    "    #     # should produce 145 files\n",
    "    #     print(i)\n",
    "    #     indices = dataset.val_video_indices[i]\n",
    "    #     name_pos = dataset.val_video_real_names[i]\n",
    "    #     types = dataset.val_video_types[i]\n",
    "    #     EX, VA, AU = split_EX_VA_AU(output[indices])\n",
    "    #     print(name_pos)\n",
    "    #     print('With ' + str(EX.shape[0]) + \" entries\")\n",
    "    #     if 'AU' in types:\n",
    "    #         write_labelfile(AU.numpy(), 'AU', name_pos, position_str=None, result_dir=o_p_v)\n",
    "    #     if 'VA' in types:\n",
    "    #         write_labelfile(VA.numpy(), 'VA', name_pos, position_str=None, result_dir=o_p_v)\n",
    "    #     if 'EX' in types:\n",
    "    #         write_labelfile(ex_from_one_hot(EX.numpy()), 'EX', name_pos, position_str=None, result_dir=o_p_v)\n",
    "    # print('done val')\n",
    "\n",
    "    # # TEST RESULTS\n",
    "    # o_p_t = os.path.join(result_path, 'test')\n",
    "    # print('writing text files test')\n",
    "    # for i in range(len(dataset.test_video_indices)):\n",
    "    #     # 14 AU\n",
    "    #     # 223 EX\n",
    "    #     # 139 VA\n",
    "    #     # = 376\n",
    "    #     print(i)\n",
    "    #     indices = dataset.test_video_indices[i]\n",
    "    #     name_pos = dataset.test_video_real_names[i]\n",
    "    #     types = dataset.test_video_types[i]\n",
    "    #     EX, VA, AU = split_EX_VA_AU(output[indices])\n",
    "    #     print(name_pos)\n",
    "    #     print('With ' + str(EX.shape[0]) + \" entries\")\n",
    "    #     if 'AU' in types:\n",
    "    #         write_labelfile(AU.numpy(), 'AU', name_pos, position_str=None, result_dir=o_p_t)\n",
    "    #     if 'VA' in types:\n",
    "    #         write_labelfile(VA.numpy(), 'VA', name_pos, position_str=None, result_dir=o_p_t)\n",
    "    #     if 'EX' in types:\n",
    "    #         write_labelfile(ex_from_one_hot(EX.numpy()), 'EX', name_pos, position_str=None, result_dir=o_p_t)\n",
    "    # print('done test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/76716 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dcd5b9db1fa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         clip_transform = NormalizeInverse(mean=[0.43216, 0.394666, 0.37645, 0.5],\n\u001b[1;32m     30\u001b[0m                                                                             std=[0.22803, 0.22145, 0.216989, 0.225])\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mclip_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-dcd5b9db1fa3>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m    \u001b[0;31m# run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# takes 5+ hours for test and val on 2080 Ti, with data on ssd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "    \"\"\"\n",
    "    Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        mean = torch.as_tensor(mean)\n",
    "        std = torch.as_tensor(std)\n",
    "        std_inv = 1 / (std + 1e-7)\n",
    "        mean_inv = -mean * std_inv\n",
    "        super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return super().__call__(tensor.clone())\n",
    "   # run inference\n",
    "    # takes 5+ hours for test and val on 2080 Ti, with data on ssd\n",
    "for data in tqdm(loader):\n",
    "        # ex_label = data['EX'].float()\n",
    "        # va_label = data['VA'].float()\n",
    "        # au_label = data['AU'].float()\n",
    "    ids = data['Index'].long()\n",
    "\n",
    "    x = {}\n",
    "    for mode in modes:\n",
    "        x[mode] = data[mode].cpu()\n",
    "        \n",
    "    for b in range(batch):\n",
    "            \n",
    "        clip_transform = NormalizeInverse(mean=[0.43216, 0.394666, 0.37645, 0.5],\n",
    "                                                                            std=[0.22803, 0.22145, 0.216989, 0.225])\n",
    "        clip_transform(x['clip'][b])\n",
    "        \n",
    "        v = x['clip'][b].permute(1,2,3,0)\n",
    "        v = v[:,:,:,:3]\n",
    "        \n",
    "        print(v)\n",
    "\n",
    "        torchvision.io.write_video(filename=f\"videos/tes2t_video{b}.mp4\",video_array=v,fps=3,video_codec='libx264')\n",
    "    print(\"done\")\n",
    "        # result = model(x)\n",
    "        # output[ids, :] = result.detach().cpu()  # output is EX VA AU\n",
    "    break\n",
    "        #labels[ids, :] = torch.cat([ex_label, va_label, au_label], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
